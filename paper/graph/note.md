### CirC Graph
* Experiment Setup:
  * The experiment is conducted across 7 benchmarks.
  * For each benchmark, we identify its main loop and define a static upperbound on its number of iteration. This upper bound is used to produce the constraints for CirC. All benchmarks except Mat Mult runs in linear time to that upperbound; Mat Mult runs in cubic time.
  * To produce the proof, we set the actual number of iterations to be 100%, 75%, and 50% of the upperbound (marked as CoBBl 100, CoBBl 75, and CoBBl 50). For each case, we repeatedly running CirC and CoBBl five times each.
  * We plot the compiler, prover, and verifier time of CoBBl as a percentage of CirC.
* Note:
  * All polynomial commitments in the tests are run in multicore, to allow CirC to compute more complex benchmarks. Note that such a construction benefits CirC more than CoBBl, as the CirC compiler & prover spends a larger fraction of time on commitments than CoBBl.
* Hypothesis:
  * Since the CoBBl compiler runs in constant time while the CirC compiler runs in at least linear time, we should expect the CoBBl compiler to run much faster in all benchmarks.
  * Since the CoBBl prover commits to a much smaller instance than CirC, it should also show significant speedup.
  * The CoBBl verifier opens a smaller instance commitment but a larger variable commitment. Since instance size grows faster than variable size, CoBBl verifier should be faster than CirC if number of iterations is large enough.
* Question:
  * Why is CoBBl compiler time not constant?  
  > XXX: This is because 100% uses for loops while 75% uses while loops

### Jolt Graph
* Experiment Setup:
  * The experiment is conducted across 7 benchmarks. All benchmarks except Poseidon are conducted on 32-bit integers, while Poseidon is conducted in 25519 fields, generated through FF.
  * To produce the proof, we encode the program as identical codes in ZoKrates (used by CoBBl) and Rust (used by Jolt), up to difference in grammatical syntax.
  * For each benchmark, we plot compiler, prover, and verifier time of both Jolt and CoBBl. We represent Poseidon in a separate graph as its Jolt runtime is on a much higher scale.
* Note:
  * Both Jolt and CoBBl are run in single core.
  * Jolt only produced a partial proof for Poseidon. The exact runtime is likely to be much slower.
* Hypothesis:
  * CoBBl compiler is specifically designed for R1CS, and thus should be much faster than the Jolt compiler.
  * The usage of lookup table in Jolt allows it to have a small commitment size, while the constraints produced by CoBBl are more concise. It is unclear how these two factors affect the prover and verifier time, but we expect the two metrics to be close for Jolt and CoBBl.
  * However, for Poseidon which performs field operations, Jolt no longer has the advantage of lookup tables, and we expect it to run much slower than CoBBl across all metrics.
  
### Benchmark Graph
* Experiment Setup:
  * The experiment is conducted on a particular benchmark, find_min. We test 8 different (upper bound of) number of iterations on the main loop from 200 to 1600, and run CirC and CoBBl on every case.
  * For each case, we plot: 
    * compiler time
    * prover time
    * verifier time
    * total number of constraints generated by the compiler (instance size)
    * number of witnesses
    * total number of constraints used in sumcheck (executed constraints)
* Note:
  * We assume that CirC's upper bound is tight, i.e. number of iterations = static bound. In reality, CirC is likely to perform less efficient.
* Hypothesis:
  * Compiler time and instance size should be constant for CoBBl, linear or superlinear for CirC.
  * CoBBl has a higher witness and executed constraint size, as it copies the same variable across multiple blocks. However, the factor should be independent of the number of iterations.
  * For prover and verifier time, poly commitment is the dominant cost. Since instance size increases faster than witness size, we expect CoBBl's prover and verifier time to grow slower than those of CirC.
* Question:
  * Why is instance size of CoBBl not an exact constant?
  * Why does the CirC compiler runs in superlinear time (makes sense to some extend, since matrix size is quadratic)

### Optimization Graph
* Experiment Setup:
  * The experiment is conducted across 7 benchmarks.
  * For each benchmark, we run on regular (optimized) CoBBl and unoptimized CoBBl, both on single core. Block Merge and Register Spilling are disabled in unoptimized CoBBl.
  * We plot the compiler, prover, and verifier time of optimized CoBBl as a percentage of unoptimized CoBBl.
  * We also plot number of blocks, number of witnesses, and number of constraints used in sumcheck of of regular CoBBl as a percentage of unoptimized CoBBl.
* Hypothesis:
  * Optimizations should generally decrease the number of blocks. Since witness commitment & opening is linear to number of blocks, this allows optimized CoBBl to outperform the unoptimized one.
  * Optimization would always increase sumcheck size (due to wasted constraints and added memory operations). However such an increase should contribute little to final runtime.
  * Depending on the effectiveness of the optimizations, number of variables may or may not decrease. This directly determines whether optimized CoBBl can outperform the unoptimized one.