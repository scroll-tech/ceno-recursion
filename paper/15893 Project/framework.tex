\section{Interpolating CirC and vRAM} \label{sec:framework}

We set up \CoBBl~by first generalizing the achievements of CirC and vRAM. CirC's optimizations are enabled by its static analysis. To retain these optimizations, \CoBBl~requires a frontend compiler. vRAM avoids constraint waste thanks to its design that permits control flow in-between its constraints -- a feature that \CoBBl~needs to incorporate.

The presence of the compiler necessitates program-specific preprocessing, while inter-constraint control flow implies that the program needs to be segmented in a way that reflects its control flow graph. It turns out there is a natural candidate for the above two requirements: compilers like LLVM \cite{lattner02llvm} perform optimizations by first dividing the code into \emph{basic blocks (BB)}. We note that expressing an execution trace in terms of BBs have the following desirable properties:
\begin{enumerate}
    \item Each BB contains no control flow in between its instructions. Thus, if the basic block were to be executed, no constraint representing it would be wasted.
    \item Typical BBs often span multiple instructions, resulting in reduced number of program states in the trace comparing to the VM approach.
    \item There exist extensive researches in the compiler community \cite{sui16svf, lee18recon} on optimizations within and between basic blocks, which can be easily incorporated into the \CoBBl~compiler.
\end{enumerate}

Despite the stated advantages, basic blocks also come with imperfections. For one, size of BBs of a program can be vastly different. Verifying the instructions of the smallest blocks might still not justify the cost of permutation and program state consistency. Furthermore, data-parallelism in the backend requires all witnesses to be of the same size, leading to extensive padding.

% Size of BBs of a program can be of vastly variable size, which translates to huge differences in number of constraints and intermediate variables. An ideal data-parallel proof, however, expects the size of constraints and witnesses to be similar across all instances.

To improve upon BBs, we observe that the trade-off between constraint wastes and program-state check is in fact a continuous spectrum, with CirC and vRAM sitting on either ends of it. While the basic block approach presents itself as a middle path between the two, it is far from being the only alternative. Since conditionals can be translated into constraints, there is no reason why program segments of a SNARK proof need to strictly follow the rules of BBs. Starting from basic blocks, \CoBBl~performs block merging and register spilling to maximize average proof efficiency. We describe details of these processes in section \ref{sec:frontend}. The optimized block are then converted into constraints, which are subsequently fed into a custom-made data-parallel version of the Spartan protocol \cite{setty19spartan}.

%\begin{itemize}
%    \item By merging or unrolling \emph{short} basic blocks, \CoBBl~reduces the number of program states at the expense of an uptick in number of wasted constraints. However, maximum constraint size remains untouched.
%    \item By spilling variables / registers in between blocks, \CoBBl~trades shortened program state width with increased memory accesses.
%\end{itemize}